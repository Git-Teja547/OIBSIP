"""
AB_NYC_2019 Data Cleaning Script
Author: Generated by ChatGPT
Description: Step-by-step Python script that loads the dataset, performs cleaning according to
the provided plan (data integrity, missing data handling, duplicate removal, standardization,
outlier detection), and saves a cleaned CSV. Suitable for submission as a project file.

Usage: Run in a Python environment with pandas and matplotlib installed.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ---------------------------
# 1. Load dataset
# ---------------------------
file_path = "/mnt/data/AB_NYC_2019.csv"  # adjust path if needed
df = pd.read_csv(file_path)

# Quick peek
print("Initial shape:", df.shape)
print(df.columns.tolist())
print(df.head(3))

# ---------------------------
# 2. Standardize column names
# ---------------------------
# Convert to lowercase and snake_case
df.columns = df.columns.str.strip().str.lower().str.replace(" ", "_")
print("\nColumns after standardization:", df.columns.tolist())

# ---------------------------
# 3. Initial integrity checks
# ---------------------------
# Null counts
null_counts = df.isnull().sum()
print("\nMissing values per column:\n", null_counts)

# Data types
print("\nData types:\n", df.dtypes)

# Basic stats for numeric columns
print("\nNumeric summary:\n", df.describe())

# ---------------------------
# 4. Handle missing data
# ---------------------------
# Strategy implemented:
# - Fill reviews_per_month NaN with 0 (no reviews)\# - Replace last_review NaN with a placeholder then convert to datetime
# - Drop rows with missing 'name' or 'host_name' (very few)

# Count missing before
missing_before = df.isnull().sum()

# fill reviews_per_month with 0
if 'reviews_per_month' in df.columns:
    df['reviews_per_month'] = df['reviews_per_month'].fillna(0)

# replace last_review NaN with 'No Reviews' placeholder for clarity, then convert to datetime
if 'last_review' in df.columns:
    df['last_review'] = df['last_review'].fillna('No Reviews')
    # convert to datetime, coerce errors so 'No Reviews' becomes NaT
    df['last_review'] = pd.to_datetime(df['last_review'], errors='coerce')

# Drop rows with missing name or host_name (very few rows)
required_cols = [c for c in ['name', 'host_name'] if c in df.columns]
df = df.dropna(subset=required_cols)

missing_after = df.isnull().sum()
print("\nMissing values before cleaning:\n", missing_before)
print("\nMissing values after cleaning:\n", missing_after)

# ---------------------------
# 5. Remove duplicates
# ---------------------------
# Prefer using unique listing id if present
initial_rows = df.shape[0]
if 'id' in df.columns:
    df = df.drop_duplicates(subset=['id'])
else:
    df = df.drop_duplicates()

duplicates_removed = initial_rows - df.shape[0]
print(f"\nDuplicates removed: {duplicates_removed}")

# ---------------------------
# 6. Data integrity validation
# ---------------------------
# 6a. Latitude / Longitude within reasonable NYC bounds
# Rough bounding box for NYC (approx): latitude 40.0-41.0, longitude -74.3 to -73.6
lat_min, lat_max = 40.0, 41.0
lon_min, lon_max = -74.3, -73.6
before_geo = df.shape[0]
if set(['latitude','longitude']).issubset(df.columns):
    df = df[df['latitude'].between(lat_min, lat_max) & df['longitude'].between(lon_min, lon_max)]

geo_removed = before_geo - df.shape[0]
print(f"Geo-invalid rows removed: {geo_removed}")

# 6b. Price > 0 and reasonably capped (choose 1000 as an example cap)
before_price = df.shape[0]
if 'price' in df.columns:
    # convert to numeric just in case
    df['price'] = pd.to_numeric(df['price'], errors='coerce')
    df = df[df['price'].notna()]
    df = df[(df['price'] > 0) & (df['price'] < 1000)]
price_removed = before_price - df.shape[0]
print(f"Price-outlier rows removed: {price_removed}")

# 6c. Minimum nights: drop extremely large values (e.g., >= 365)
before_min_nights = df.shape[0]
if 'minimum_nights' in df.columns:
    df['minimum_nights'] = pd.to_numeric(df['minimum_nights'], errors='coerce')
    df = df[df['minimum_nights'] < 365]
min_nights_removed = before_min_nights - df.shape[0]
print(f"Minimum-nights outlier rows removed: {min_nights_removed}")

# 6d. availability_365 must be between 0 and 365
before_avail = df.shape[0]
if 'availability_365' in df.columns:
    df['availability_365'] = pd.to_numeric(df['availability_365'], errors='coerce')
    df = df[df['availability_365'].between(0,365)]
avail_removed = before_avail - df.shape[0]
print(f"Availability-outlier rows removed: {avail_removed}")

# ---------------------------
# 7. Final housekeeping
# ---------------------------
# Standardize categorical columns (strip whitespace, unify case)
cat_cols = df.select_dtypes(include=['object']).columns
for c in cat_cols:
    df[c] = df[c].astype(str).str.strip()

# Recompute final shape
final_rows, final_cols = df.shape
print(f"\nFinal dataset shape: {final_rows} rows, {final_cols} columns")

# Summary dictionary to export
summary = {
    'initial_rows': 48895,
    'after_missing_name_host': df.shape[0] + duplicates_removed + geo_removed + price_removed + min_nights_removed + avail_removed, # approximate checkpoint
    'duplicates_removed': int(duplicates_removed),
    'geo_removed': int(geo_removed),
    'price_removed': int(price_removed),
    'min_nights_removed': int(min_nights_removed),
    'availability_removed': int(avail_removed),
    'final_rows': int(final_rows),
    'final_columns': int(final_cols)
}

print("\nCleaning summary:\n", summary)

# ---------------------------
# 8. Save cleaned dataset
# ---------------------------
clean_path = "/mnt/data/AB_NYC_2019_Cleaned.csv"
df.to_csv(clean_path, index=False)
print(f"\nCleaned dataset saved to: {clean_path}")

# ---------------------------
# 9. Optional: Small visualizations for submission report
# ---------------------------
# Price distribution (histogram)
if 'price' in df.columns:
    plt.figure(figsize=(8,4))
    plt.hist(df['price'], bins=50)
    plt.title('Price distribution (cleaned)')
    plt.xlabel('Price')
    plt.ylabel('Count')
    plt.tight_layout()
    plt.savefig('/mnt/data/price_distribution_cleaned.png')
    print('Saved price distribution plot to /mnt/data/price_distribution_cleaned.png')

# Room type counts bar chart
if 'room_type' in df.columns:
    room_counts = df['room_type'].value_counts()
    plt.figure(figsize=(6,3))
    room_counts.plot(kind='bar')
    plt.title('Room type counts (cleaned)')
    plt.xlabel('Room type')
    plt.ylabel('Count')
    plt.tight_layout()
    plt.savefig('/mnt/data/room_type_counts.png')
    print('Saved room type counts plot to /mnt/data/room_type_counts.png')

# ---------------------------
# 10. Export summary to CSV for your report
# ---------------------------
pd.DataFrame([summary]).to_csv('/mnt/data/cleaning_summary.csv', index=False)
print('Saved cleaning summary to /mnt/data/cleaning_summary.csv')

# End of script
print('\nAll done â€” include this script and the generated CSV/plots in your submission.')
S